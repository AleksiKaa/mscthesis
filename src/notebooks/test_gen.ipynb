{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb1635b5-1774-4850-b0cb-aa1a60383482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set environment variables before importing transformers\n",
    "os.environ[\"HUGGINGFACE_HUB_CACHE\"] = \"/scratch/shareddata/dldata/huggingface-hub-cache/hub\" # Load local models\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\" \n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = f\"{os.environ[\"WRKDIR\"]}/.cache/huggingface\" # Cache in work directory\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    ")  # Add parent directory to path\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "\n",
    "from utils.prompts import (\n",
    "    JUDGE_SYSTEM_PROMPT,\n",
    "    JUDGE_TEMPLATE,\n",
    "    GENERATE_EXERCISES_SYSTEM_PROMPT,\n",
    "    GENERATE_EXERCISES_TEMPLATE_EXPLICIT,\n",
    "    GENERATE_EXERCISES_TEMPLATE_FEWSHOT,\n",
    "    GENERATE_EXERCISES_TEMPLATE_IMPLICIT,\n",
    "    GENERATE_EXERCISES_TEMPLATE_ZEROSHOT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63819a30-71dd-4097-9b8f-955ff90c1026",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_DATA = r\"../../data/complete_dataset.csv\"\n",
    "DEFAULT_MODEL = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "DEFAULT_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "system_prompts = {\n",
    "    \"judge\": JUDGE_SYSTEM_PROMPT,\n",
    "    \"zeroshot\": GENERATE_EXERCISES_SYSTEM_PROMPT,\n",
    "    \"fewshot\": GENERATE_EXERCISES_SYSTEM_PROMPT,\n",
    "    \"explicit\": GENERATE_EXERCISES_SYSTEM_PROMPT,\n",
    "    \"implicit\": GENERATE_EXERCISES_SYSTEM_PROMPT,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aefac211-9560-4ac8-9849-0c6994fa6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(row, task_type):\n",
    "    _, problem_description, example_solution, _, _, theme, topic, concept, *_ = row\n",
    "\n",
    "    match task_type:\n",
    "        case \"judge\":\n",
    "                return (JUDGE_TEMPLATE.replace(\"$THEME$\", theme)\n",
    "                .replace(\"$TOPIC$\", topic)\n",
    "                .replace(\"$CONCEPT$\", concept)\n",
    "                .replace(\"$TEXT$\", problem_description)\n",
    "                .replace(\"$CODE$\", example_solution)\n",
    "            )\n",
    "        case \"zeroshot\":\n",
    "            return GENERATE_EXERCISES_TEMPLATE_ZEROSHOT\n",
    "        case \"fewshot\":\n",
    "            return GENERATE_EXERCISES_TEMPLATE_FEWSHOT\n",
    "        case \"explicit\":\n",
    "            return GENERATE_EXERCISES_TEMPLATE_EXPLICIT\n",
    "        case \"implicit\":\n",
    "            return GENERATE_EXERCISES_TEMPLATE_IMPLICIT\n",
    "        case _:\n",
    "            raise ValueError(f\"Task type '{_}' not recognised as valid task type!\")\n",
    "\n",
    "\n",
    "def run_model(pipe, data, task_type):\n",
    "    system_prompt = system_prompts.get(task_type, None)\n",
    "\n",
    "    if system_prompt is None:\n",
    "        raise ValueError(f\"Task type '{task_type}' not recognised as valid task type!\")\n",
    "\n",
    "    response = pipe(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": data[\"prompt\"]},\n",
    "        ],\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=500,\n",
    "    )\n",
    "\n",
    "    result = response[0][\"generated_text\"]\n",
    "    result_dict = json.loads(result)\n",
    "\n",
    "    for k, v in result_dict.items():\n",
    "        data[k] = v\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c03a9d92-d491-4d47-ac27-0ad881d00537",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"judge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b1f6344-aace-4acd-a6b3-2a723ba0895e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca54ca9ea10c4699a6be48c511c884ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"model\": DEFAULT_MODEL,\n",
    "    \"device_map\": 0,  # Force GPU\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"temperature\": 0.3,\n",
    "}\n",
    "\n",
    "pipeline = transformers.pipeline(\"text-generation\", **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44a27dea-69bd-46e8-b38f-77f94b198b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4039796/706892912.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_df[\"prompt\"] = eval_df.apply(lambda row: make_prompt(row, task), axis=1)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DEFAULT_DATA, sep=\";\")\n",
    "n_rows = 3\n",
    "\n",
    "eval_df = df.loc[0:n_rows - 1]\n",
    "eval_df[\"prompt\"] = eval_df.apply(lambda row: make_prompt(row, task), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fafee9a3-98cc-4fbe-9d6c-78ecefe92d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "result = eval_df.apply(lambda row: run_model(pipeline, row, task), axis=1)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a9dfeff-fdcd-40fb-9f6e-877da3cd9569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation:  yes\n",
      "Reasoning:  The exercise is about creating a program that takes a user input, a novel rating, and prints the corresponding textual description. It aligns with the theme (literature), topic (Agatha Christie), and programming concept (conditional statements). The provided solution correctly implements the required functionality.\n",
      "\n",
      "Evaluation:  yes\n",
      "Reasoning:  The exercise is well-structured and aligns with the intended theme (handicrafts), topic (painting), and programming concept (conditional statements). The provided solution correctly asks for user input, parses it as an integer, and uses if-else statements to print the corresponding description based on the input. The only improvement could be adding more error handling for non-numeric input.\n",
      "\n",
      "Evaluation:  yes\n",
      "Reasoning:  This exercise is about writing a simple Dart program that asks for user input and prints a message based on that input. The program aligns with the theme (food) and topic (Lingonberry sauce). The programming concept is user input, which is correctly implemented in the provided solution.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, row in result.iterrows():\n",
    "    print(\"Evaluation: \", row[\"Correct\"])\n",
    "    print(\"Reasoning: \", row[\"Explanation\"])\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python LLM env (scicomp-llm-env)",
   "language": "python",
   "name": "python3-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
