@article{10.1145/3703155,
author = {Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
title = {A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
year = {2025},
issue_date = {March 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {2},
issn = {1046-8188},
url = {https://doi.org/10.1145/3703155},
doi = {10.1145/3703155},
abstract = {The emergence of large language models (LLMs) has marked a significant breakthrough in natural language processing (NLP), fueling a paradigm shift in information acquisition. Nevertheless, LLMs are prone to hallucination, generating plausible yet nonfactual content. This phenomenon raises significant concerns over the reliability of LLMs in real-world information retrieval (IR) systems and has attracted intensive research to detect and mitigate such hallucinations. Given the open-ended general-purpose attributes inherent to LLMs, LLM hallucinations present distinct challenges that diverge from prior task-specific models. This divergence highlights the urgency for a nuanced understanding and comprehensive overview of recent advances in LLM hallucinations. In this survey, we begin with an innovative taxonomy of hallucination in the era of LLM and then delve into the factors contributing to hallucinations. Subsequently, we present a thorough overview of hallucination detection methods and benchmarks. Our discussion then transfers to representative methodologies for mitigating LLM hallucinations. Additionally, we delve into the current limitations faced by retrieval-augmented LLMs in combating hallucinations, offering insights for developing more robust IR systems. Finally, we highlight the promising research directions on LLM hallucinations, including hallucination in large vision-language models and understanding of knowledge boundaries in LLM hallucinations.},
journal = {ACM Trans. Inf. Syst.},
month = jan,
articleno = {42},
numpages = {55},
keywords = {Large Language Models, Hallucination, Factuality, Faithfulness}
}

@ARTICLE{Minaee2024-fr,
  title         = "Large Language Models: A survey",
  author        = "Minaee, Shervin and Mikolov, Tomas and Nikzad, Narjes and
                   Chenaghlu, Meysam and Socher, Richard and Amatriain, Xavier
                   and Gao, Jianfeng",
  abstract      = "Large Language Models (LLMs) have drawn a lot of attention
                   due to their strong performance on a wide range of natural
                   language tasks, since the release of ChatGPT in November
                   2022. LLMs' ability of general-purpose language
                   understanding and generation is acquired by training
                   billions of model's parameters on massive amounts of text
                   data, as predicted by scaling laws
                   \textbackslashcite\{kaplan2020scaling,hoffmann2022training\}.
                   The research area of LLMs, while very recent, is evolving
                   rapidly in many different ways. In this paper, we review
                   some of the most prominent LLMs, including three popular LLM
                   families (GPT, LLaMA, PaLM), and discuss their
                   characteristics, contributions and limitations. We also give
                   an overview of techniques developed to build, and augment
                   LLMs. We then survey popular datasets prepared for LLM
                   training, fine-tuning, and evaluation, review widely used
                   LLM evaluation metrics, and compare the performance of
                   several popular LLMs on a set of representative benchmarks.
                   Finally, we conclude the paper by discussing open challenges
                   and future research directions.",
  month         =  feb,
  year          =  2024,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2402.06196"
}

@inproceedings{dong-etal-2024-survey,
    title = "A Survey on In-context Learning",
    author = "Dong, Qingxiu  and
      Li, Lei  and
      Dai, Damai  and
      Zheng, Ce  and
      Ma, Jingyuan  and
      Li, Rui  and
      Xia, Heming  and
      Xu, Jingjing  and
      Wu, Zhiyong  and
      Chang, Baobao  and
      Sun, Xu  and
      Li, Lei  and
      Sui, Zhifang",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.64/",
    doi = "10.18653/v1/2024.emnlp-main.64",
    pages = "1107--1128",
    abstract = "With the increasing capabilities of large language models (LLMs), in-context learning (ICL) has emerged as a new paradigm for natural language processing (NLP), where LLMs make predictions based on contexts augmented with a few examples. It has been a significant trend to explore ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress and challenges of ICL. We first present a formal definition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques, including training strategies, prompt designing strategies, and related analysis. Additionally, we explore various ICL application scenarios, such as data engineering and knowledge updating. Finally, we address the challenges of ICL and suggest potential directions for further research. We hope that our work can encourage more research on uncovering how ICL works and improving ICL."
}

@ARTICLE{Kalai2025-fl,
  title         = "Why language models hallucinate",
  author        = "Kalai, Adam Tauman and Nachum, Ofir and Vempala, Santosh S
                   and Zhang, Edwin",
  abstract      = "Like students facing hard exam questions, large language
                   models sometimes guess when uncertain, producing plausible
                   yet incorrect statements instead of admitting uncertainty.
                   Such ``hallucinations'' persist even in state-of-the-art
                   systems and undermine trust. We argue that language models
                   hallucinate because the training and evaluation procedures
                   reward guessing over acknowledging uncertainty, and we
                   analyze the statistical causes of hallucinations in the
                   modern training pipeline. Hallucinations need not be
                   mysterious -- they originate simply as errors in binary
                   classification. If incorrect statements cannot be
                   distinguished from facts, then hallucinations in pretrained
                   language models will arise through natural statistical
                   pressures. We then argue that hallucinations persist due to
                   the way most evaluations are graded -- language models are
                   optimized to be good test-takers, and guessing when
                   uncertain improves test performance. This ``epidemic'' of
                   penalizing uncertain responses can only be addressed through
                   a socio-technical mitigation: modifying the scoring of
                   existing benchmarks that are misaligned but dominate
                   leaderboards, rather than introducing additional
                   hallucination evaluations. This change may steer the field
                   toward more trustworthy AI systems.",
  month         =  sep,
  year          =  2025,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL",
  eprint        = "2509.04664"
}

@inproceedings{alazraki-etal-2025-need,
    title = "No Need for Explanations: {LLM}s can implicitly learn from mistakes in-context",
    author = "Alazraki, Lisa  and
      Mozes, Maximilian  and
      Campos, Jon Ander  and
      Yi-Chern, Tan  and
      Rei, Marek  and
      Bartolo, Max",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.emnlp-main.1686/",
    doi = "10.18653/v1/2025.emnlp-main.1686",
    pages = "33179--33203",
    ISBN = "979-8-89176-332-6",
    abstract = "Showing incorrect answers to Large Language Models (LLMs) is a popular strategy to improve their performance in reasoning-intensive tasks. It is widely assumed that, in order to be helpful, the incorrect answers must be accompanied by comprehensive rationales, explicitly detailing where the mistakes are and how to correct them. However, in this work we present a counterintuitive finding: we observe that LLMs perform *better* in math reasoning tasks when these rationales are eliminated from the context and models are left to infer on their own what makes an incorrect answer flawed. This approach also substantially outperforms chain-of-thought prompting in our evaluations. These results are consistent across LLMs of different sizes and varying reasoning abilities. To gain an understanding of *why* LLMs learn from mistakes more effectively without explicit corrective rationales, we perform a thorough analysis, investigating changes in context length and answer diversity between different prompting strategies, and their effect on performance. We also examine evidence of overfitting to the in-context rationales when these are provided, and study the extent to which LLMs are able to autonomously infer high-quality corrective rationales given only incorrect answers as input. We find evidence that, while incorrect answers are more beneficial for LLM learning than additional diverse *correct* answers, explicit corrective rationales over-constrain the model, thus limiting those benefits."
}

@misc{an2024learningmistakesmakesllm,
      title={Learning From Mistakes Makes LLM Better Reasoner}, 
      author={Shengnan An and Zexiong Ma and Zeqi Lin and Nanning Zheng and Jian-Guang Lou and Weizhu Chen},
      year={2024},
      eprint={2310.20689},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.20689}, 
}

@inproceedings{10.1145/3632620.3671103,
author = {Logacheva, Evanfiya and Hellas, Arto and Prather, James and Sarsa, Sami and Leinonen, Juho},
title = {Evaluating Contextually Personalized Programming Exercises Created with Generative AI},
year = {2024},
isbn = {9798400704758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632620.3671103},
doi = {10.1145/3632620.3671103},
abstract = {Programming skills are typically developed through completing various hands-on exercises. Such programming problems can be contextualized to students’ interests and cultural backgrounds. Prior research in educational psychology has demonstrated that context personalization of exercises stimulates learners’ situational interests and positively affects their engagement. However, creating a varied and comprehensive set of programming exercises for students to practice on is a time-consuming and laborious task for computer science educators. Previous studies have shown that large language models can generate conceptually and contextually relevant programming exercises. Thus, they offer a possibility to automatically produce personalized programming problems to fit students’ interests and needs. This article reports on a user study conducted in an elective introductory programming course that included contextually personalized programming exercises created with GPT-4. The quality of the exercises was evaluated by both the students and the authors. Additionally, this work investigated student attitudes towards the created exercises and their engagement with the system. The results demonstrate that the quality of exercises generated with GPT-4 was generally high. What is more, the course participants found them engaging and useful. This suggests that AI-generated programming problems can be a worthwhile addition to introductory programming courses, as they provide students with a practically unlimited pool of practice material tailored to their personal interests and educational needs.},
booktitle = {Proceedings of the 2024 ACM Conference on International Computing Education Research - Volume 1},
pages = {95–113},
numpages = {19},
keywords = {automatic exercise generation, context personalization, generative AI, large language models},
location = {Melbourne, VIC, Australia},
series = {ICER '24}
}